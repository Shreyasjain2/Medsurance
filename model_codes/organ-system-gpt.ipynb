{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":6156362,"datasetId":3531470,"databundleVersionId":6235286}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport kagglehub\n\n# Download latest version\n# path = kagglehub.dataset_download(\"allanwandia/sepsis\")\ndf = pd.read_csv(\"/kaggle/input/sepsis/Paitients_Files_Train - Paitients_Files_Train.csv\")  # Adjust filename if needed\n\n# Remove units from numerical columns\ndef remove_units(value):\n    return float(re.findall(r\"[\\d.]+\", str(value))[0]) if re.search(r\"[\\d.]+\", str(value)) else np.nan\n\ncolumns_with_units = [\"SK\", \"TS\", \"PR\", \"PL\", \"M11\", \"BD2\"]  # List of columns to clean\nfor col in columns_with_units:\n    df[col] = df[col].apply(remove_units)\n\n# Convert the target column to binary (0 = Negative, 1 = Positive)\ndf[\"Sepssis\"] = df[\"Sepssis\"].map({\"Negative\": 0, \"Positive\": 1})\ndf= df.drop(columns=[\"ID\", \"Insurance\"])\n# Handle missing values (impute with mean)\ndf.fillna(df.mean(), inplace=True)\n\n# Split dataset into features (X) and target (y)\nX = df.drop(columns=[\"Sepssis\"])\ny = df[\"Sepssis\"]\n\n# Normalize the data\n\nprint(X)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import keras\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\n# Build the ANN model\nmodel = keras.Sequential([\n    keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32)\n\n# Evaluate\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test Accuracy: {accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T16:16:31.332007Z","iopub.execute_input":"2025-02-22T16:16:31.332319Z","iopub.status.idle":"2025-02-22T16:16:36.367512Z","shell.execute_reply.started":"2025-02-22T16:16:31.332289Z","shell.execute_reply":"2025-02-22T16:16:36.366548Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - accuracy: 0.5137 - loss: 0.6803 - val_accuracy: 0.7167 - val_loss: 0.6005\nEpoch 2/20\n\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7378 - loss: 0.5672 - val_accuracy: 0.7000 - val_loss: 0.5600\nEpoch 3/20\n\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7831 - loss: 0.4878 - val_accuracy: 0.7167 - val_loss: 0.5658\nEpoch 4/20\n\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7675 - loss: 0.4732 - val_accuracy: 0.7667 - val_loss: 0.5729\nEpoch 5/20\n\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8069 - loss: 0.4187 - val_accuracy: 0.7583 - val_loss: 0.5789\nEpoch 6/20\n\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7965 - loss: 0.4281 - val_accuracy: 0.7583 - val_loss: 0.5642\nEpoch 7/20\n\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8011 - loss: 0.4292 - val_accuracy: 0.7667 - val_loss: 0.5629\nEpoch 8/20\n\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7950 - loss: 0.4208 - val_accuracy: 0.7333 - val_loss: 0.5801\nEpoch 9/20\n\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8174 - loss: 0.4200 - val_accuracy: 0.7583 - val_loss: 0.5760\nEpoch 10/20\n\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7999 - loss: 0.4037 - val_accuracy: 0.7667 - val_loss: 0.5819\nEpoch 11/20\n\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8235 - loss: 0.3857 - val_accuracy: 0.7250 - val_loss: 0.5863\nEpoch 12/20\n\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7943 - loss: 0.4071 - val_accuracy: 0.7583 - val_loss: 0.5858\nEpoch 13/20\n\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8058 - loss: 0.4051 - val_accuracy: 0.7667 - val_loss: 0.5755\nEpoch 14/20\n\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8115 - loss: 0.4210 - val_accuracy: 0.7250 - val_loss: 0.5975\nEpoch 15/20\n\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8065 - loss: 0.3900 - val_accuracy: 0.7500 - val_loss: 0.5957\nEpoch 16/20\n\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8146 - loss: 0.3626 - val_accuracy: 0.7417 - val_loss: 0.6087\nEpoch 17/20\n\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8241 - loss: 0.3572 - val_accuracy: 0.7250 - val_loss: 0.6101\nEpoch 18/20\n\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8051 - loss: 0.3917 - val_accuracy: 0.7500 - val_loss: 0.5973\nEpoch 19/20\n\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8171 - loss: 0.3692 - val_accuracy: 0.7000 - val_loss: 0.6233\nEpoch 20/20\n\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8252 - loss: 0.3704 - val_accuracy: 0.7417 - val_loss: 0.6098\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7227 - loss: 0.6418 \nTest Accuracy: 0.7417\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Train an XGBoost Decision Tree Model\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\n# Define XGBoost model\nxgb_model = xgb.XGBClassifier(\n    objective='binary:logistic', \n    eval_metric='logloss',\n    use_label_encoder=False,\n    tree_method='gpu_hist'\n)\n\n# Define parameter grid\nparam_grid = {\n    'max_depth': [3, 5, 7],              # Tree depth\n    'learning_rate': [0.01, 0.1],   # Step size\n    'n_estimators': [100, 200],     # Number of trees\n    'subsample': [0.7, 0.8, 1.0],        # Fraction of samples per tree\n    'colsample_bytree': [0.7, 0.8, 1.0], # Feature sampling\n    'gamma': [0.1, 0.2]               # Regularization\n}\n\n# Grid Search with 5-Fold Cross-Validation\ngrid_search = GridSearchCV(\n    estimator=xgb_model, \n    param_grid=param_grid, \n    scoring='accuracy', \n    cv=5, \n    verbose=1, \n    n_jobs=-1\n)\n\n# Fit model\ngrid_search.fit(X_train, y_train)\n\n# Best parameters and best model\nbest_params = grid_search.best_params_\nbest_model = grid_search.best_estimator_\n\nprint(\"Best Hyperparameters:\", best_params)\n\n# Make predictions\ny_pred = best_model.predict(X_test)\n\n# Evaluate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Tuned XGBoost Test Accuracy: {accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:52:19.089458Z","iopub.execute_input":"2025-02-22T15:52:19.089802Z","iopub.status.idle":"2025-02-22T16:02:17.024275Z","shell.execute_reply.started":"2025-02-22T15:52:19.089771Z","shell.execute_reply":"2025-02-22T16:02:17.023264Z"}},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [16:02:16] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Best Hyperparameters: {'colsample_bytree': 0.8, 'gamma': 0.2, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.7}\nTuned XGBoost Test Accuracy: 0.7167\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [16:02:16] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}